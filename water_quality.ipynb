{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data & Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/MRE/Documents/GitHub/Water_Qualityy/water_potability.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Names and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ph'] = data['ph'].fillna(data['ph'].mean())\n",
    "data['Sulfate'] = data['Sulfate'].fillna(data['Sulfate'].mean())\n",
    "data['Trihalomethanes'] = data['Trihalomethanes'].fillna(data['Trihalomethanes'].mean())\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(bins=30, figsize=(20, 15))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "sns.boxplot(data=data)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Analysis for Individual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.columns\n",
    "plt.figure(figsize= (20, 15))\n",
    "\n",
    "for i, feature in enumerate(features, 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.boxplot(data[feature])\n",
    "    plt.title(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "for i, feature in enumerate(features[:-1], 1):  # Except Potability feature\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.boxplot(x='Potability', y=feature, data=data)\n",
    "    plt.title(f'Potability vs {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.corr()\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "sns.heatmap(corr_matrix, annot = True, cmap = \"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = data.drop('Potability', axis = 1)\n",
    "y = data['Potability']\n",
    "\n",
    "#Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#Normalization\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Checking results\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "#DTC Modelling\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "#Prediction\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "#Performance Evaluation\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Hyperparameter Grid for DTC\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "\n",
    "#DTC Modelling\n",
    "dt_model = DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "#Hyperparameter Tuning with GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = dt_model,\n",
    "    param_grid = param_grid,\n",
    "    cv = 5,\n",
    "    n_jobs = -1,\n",
    "    scoring = 'accuracy'\n",
    ")\n",
    "\n",
    "# Appyling GridSearch on Train Data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# For the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {best_score}\")\n",
    "\n",
    "# Applying with Best of's on Train Data\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "#Performance Evaluation\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Despite the expanded hyperparameter grid, we see that the best parameters and model performance are the same as previous results. This indicates that the current hyperparameter range is sufficient and we should try other methods to further improve the performance of the model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Hyperparameter Grid for Random Forest Model\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "#Random Forest Model\n",
    "rf_model = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "# Hyperparameter Tuning with GridSearchCV\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid = param_grid_rf,\n",
    "    cv = 5,\n",
    "    n_jobs = 1,\n",
    "    scoring = 'accuracy'\n",
    ")\n",
    "\n",
    "# Execure the grid on train data\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Getting best parameters and scores\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_score_rf = grid_search_rf.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params_rf}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {best_score_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation with Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_rf = grid_search_rf.best_estimator_\n",
    "y_pred_rf = best_model_rf.predict(X_test)\n",
    "\n",
    "#Performance Evaluation\n",
    "# Performans değerlendirmesi\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b>Conclusion:</b>\n",
    "Improvement: Random Forest model performed better compared to Decision Tree model. The overall accuracy rate and the rate of correctly predicting potable water (recall) have increased.\n",
    "Weakness: The rate of correctly predicting potable water is still low, but the Random Forest model gave better results than Decision Tree in this regard.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Hyperparameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid_rf = {\n",
    "#     'n_estimators': [100, 200, 300, 500],\n",
    "#     'max_depth': [5, 10, 15, 20, None],\n",
    "#     'min_samples_split': [2, 5, 10, 15],\n",
    "#     'min_samples_leaf': [1, 2, 4, 6],\n",
    "#     'criterion': ['gini', 'entropy']\n",
    "# }\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'criterion': ['gini']\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator = rf_model,\n",
    "    param_grid = param_grid_rf,\n",
    "    cv = 5,\n",
    "    n_jobs = 1,\n",
    "    scoring = 'accuracy'\n",
    ")\n",
    "\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# For the best Parameter & Scores\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_scores_rf = grid_search_rf.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params_rf}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {best_score_rf}\")\n",
    "\n",
    "# Implementing with best estimators\n",
    "best_model_rf = grid_search_rf.best_estimator_\n",
    "y_pred_rf = best_model_rf.predict(X_test)\n",
    "\n",
    "#Performance Evaluation\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>When we examine the results, we see that they are the same as the previous results. This shows that the performance of our model is limited even with the extended hyperparameter grid.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "#Logistic Regression Model\n",
    "log_reg_model = LogisticRegression(random_state = 42, solver = 'liblinear')\n",
    "\n",
    "# Hyperparameter grid for Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Hyperparameter Settings with GridSearchCV\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator = log_reg_model,\n",
    "    param_grid = param_grid_lr,\n",
    "    cv = 5,\n",
    "    n_jobs = 1,\n",
    "    scoring = 'accuracy'\n",
    ")\n",
    "\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "# For the best Parameter & Scores\n",
    "best_params_lr = grid_search_lr.best_params_\n",
    "best_scores_lr = grid_search_lr.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params_lr}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {best_scores_lr}\")\n",
    "\n",
    "# Implementing with best estimators\n",
    "best_model_lr = grid_search_lr.best_estimator_\n",
    "y_pred_lr = best_model_lr.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>These results show that the Logistic Regression model cannot accurately predict potable water (Class 1) in the data set.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>General evaluation:</p>\n",
    "<p>Decision Tree Classifier: Performed moderately. It remained weak in its definitions of potable water.</p>\n",
    "<p>Random Forest Classifier: Achieved the highest accuracy ranges, but still performed poorly at identifying potable water.</p>\n",
    "<p>Logistic Regression: Potable water was never identified due to class imbalance. This model showed poor performance.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
